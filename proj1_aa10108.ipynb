{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b043b951-dc9f-4342-bffc-d34899758406",
   "metadata": {},
   "source": [
    "# Abdullah Alhussni - aa10108\n",
    "## Applied Machine Learning - ENGR-UH 3332 - Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d3222-e94d-4be5-aa8a-f914376e2f98",
   "metadata": {},
   "source": [
    "### 1 + 2 - Importing the data set and setting up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "89c55a85-a67d-455f-afcc-8e1c22519d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## Avoid printing out warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)\n",
    "\n",
    "a = 2\n",
    "r = 0.1\n",
    "\n",
    "def gradient(X, y, y_pred, theta):\n",
    "\treturn (\n",
    "\t\t- (2 / y.size) * X.T.dot(y - y_pred) # gradient of mse\n",
    "\t)\n",
    "\n",
    "def lasso_gradient(X, y, y_pred, theta):\n",
    "\treturn (\n",
    "\t\t- (2 / y.size) * X.T.dot(y - y_pred) # gradient of mse\n",
    "\t\t+ a * np.sign(theta)\t\t         # subgradient of 11-penalty\n",
    "\t)\n",
    "\n",
    "def elastic_gradient(X, y, y_pred, theta):\n",
    "\treturn (\n",
    "\t\t- (2 / y.size) * X.T.dot(y - y_pred) # gradient of mse\n",
    "\t\t+ a * np.sign(theta)\t\t         # subgradient of 11-penalty\n",
    "        + a * (1 - r) * theta \t\t\t     # gradient of 12-penalty\n",
    "\t)\n",
    "\n",
    "def find_theta(X, y, l): # for both regular and ridge regression\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    a = np.append(X, np.ones((m, 1)), axis = 1)\n",
    "    return np.dot(np.linalg.pinv(np.dot(X.T, X) + l * np.identity(n)), np.dot(X.T, y))\n",
    "\n",
    "def predict(X, theta):\n",
    "    m = X.shape[0]\n",
    "    a = np.append(X, np.ones((m, 1)), axis = 1)\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b6493-8174-44d8-b5fb-30f5b76ffb40",
   "metadata": {},
   "source": [
    "### 3 - Linear regression, closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8c4ab691-91b3-413d-9280-23710469e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.32705988271211\n"
     ]
    }
   ],
   "source": [
    "theta3 = find_theta(X, y, 0)\n",
    "y_pred3 = predict(X, theta3)\n",
    "\n",
    "k = 7\n",
    "\n",
    "kf = KFold(n_splits = k)\n",
    "\n",
    "kf3 = np.column_stack((X, y))\n",
    "\n",
    "scores3 = []\n",
    "for train, test in kf.split(kf3):\n",
    "    X_train = kf3[train, :-1]\n",
    "    y_train = kf3[train, -1]\n",
    "    X_test = kf3[test, :-1]\n",
    "    y_test = kf3[test, -1]\n",
    "\n",
    "    theta_temp = find_theta(X_train, y_train, 0)\n",
    "    y_pred_temp = predict(X_test, theta_temp)\n",
    "    diff = y_pred_temp - y_test\n",
    "    MSE = sum(diff ** 2) / diff.size\n",
    "    scores3.append(MSE)\n",
    "\n",
    "print(np.average(scores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c19c3-da51-4712-9c72-081db8a7ec2c",
   "metadata": {},
   "source": [
    "### 4 - Ridge regression, closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "dd7da9b7-e6ac-4972-8a6f-06d12f54b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8710292667754\n",
      "33.657553485206826\n",
      "34.857971150261946\n",
      "41.34735547311392\n",
      "51.95142222059484\n",
      "60.41843532305361\n",
      "67.55922935851414\n",
      "74.76694253933445\n",
      "80.3821378200417\n",
      "84.63246275865025\n",
      "88.13311797809762\n",
      "93.46530228333746\n",
      "109.26108023258048\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(1, 7, num = 13)\n",
    "\n",
    "theta4 = np.zeros((X.shape[0], lambdas.size))\n",
    "y_pred4 = np.zeros((X.shape[1], lambdas.size))\n",
    "\n",
    "scores4 = np.zeros((k, lambdas.size))\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    for j, (train, test) in enumerate(kf.split(kf3)):\n",
    "        X_train = kf3[train, :-1]\n",
    "        y_train = kf3[train, -1]\n",
    "        X_test = kf3[test, :-1]\n",
    "        y_test = kf3[test, -1]\n",
    "    \n",
    "        theta_temp = find_theta(X_train, y_train, lambdas[i])\n",
    "        y_pred_temp = predict(X_test, theta_temp)\n",
    "        diff = y_pred_temp - y_test\n",
    "        MSE = sum(diff ** 2) / diff.size\n",
    "        scores4[j, i] = MSE\n",
    "    print(np.average(scores4[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6894c-1f69-4d98-8876-7b10cd86dd2c",
   "metadata": {},
   "source": [
    "As we can see, lambda = 10^1.5 = 10sqrt(10) = 31.6 gives the lowest MSE (in fact, most values between 1 and 100 give similar MSE, hence lambda = 10 is also a safe choice) and hence the least error. Trying lambda < 10 does not give better results, as MSE does not seem to go below 33.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f138ae-629f-4c51-aac5-ed1dd1b08f94",
   "metadata": {},
   "source": [
    "### 5 - Ridge regression, optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "90672286-9f6b-40b1-902f-442e2aec508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.657553485206826\n"
     ]
    }
   ],
   "source": [
    "lambda5 = 10 ** 1.5\n",
    "\n",
    "theta5 = find_theta(X, y, lambda5)\n",
    "y_pred5 = predict(X, theta5)\n",
    "\n",
    "scores5 = []\n",
    "for train, test in kf.split(kf3):\n",
    "    X_train = kf3[train, :-1]\n",
    "    y_train = kf3[train, -1]\n",
    "    X_test = kf3[test, :-1]\n",
    "    y_test = kf3[test, -1]\n",
    "\n",
    "    theta_temp = find_theta(X_train, y_train, lambda5)\n",
    "    y_pred_temp = predict(X_test, theta_temp)\n",
    "    diff = y_pred_temp - y_test\n",
    "    MSE = sum(diff ** 2) / diff.size\n",
    "    scores5.append(MSE)\n",
    "\n",
    "print(np.average(scores5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0567500-cb9f-42ca-8d13-ec0534ca303d",
   "metadata": {},
   "source": [
    "### 6 - Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cf269ea0-139d-422d-93b6-8078b99aeec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.32452022742493\n",
      "59.63938477118783\n",
      "47.871789932177414\n",
      "42.25510311460955\n",
      "35.303492509184835\n",
      "28.610349447086495\n",
      "28.156289582154177\n",
      "30.15353681290134\n",
      "32.71503922653546\n",
      "37.32136948595105\n",
      "42.595151451910674\n",
      "46.53114246921746\n",
      "46.384473402275276\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "kf6 = np.column_stack((X_poly, y))\n",
    "\n",
    "theta6 = np.zeros((X_poly.shape[1], lambdas.size))\n",
    "y_pred6 = np.zeros((X_poly.shape[0], lambdas.size))\n",
    "\n",
    "scores6 = np.zeros((k, lambdas.size))\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    theta6[:, i] = find_theta(X_poly, y, lambdas[i])\n",
    "    y_pred6[:, i] = predict(X_poly, theta6[:, i])\n",
    "    \n",
    "    for j, (train, test) in enumerate(kf.split(kf6)):\n",
    "        X_train = kf6[list(train), :-1]\n",
    "        y_train = kf6[list(train), -1]\n",
    "        X_test = kf6[list(test), :-1]\n",
    "        y_test = kf6[list(test), -1]\n",
    "\n",
    "        theta_temp = find_theta(X_train, y_train, lambdas[i])\n",
    "        y_pred = predict(X_test, theta_temp)\n",
    "        diff = y_test - y_pred\n",
    "        MSE = np.sum(diff ** 2) / diff.size\n",
    "        scores6[j, i] = MSE\n",
    "    \n",
    "    print(np.average(scores6[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44003ef-afac-4f47-962d-88b0ae042b26",
   "metadata": {},
   "source": [
    "Here the lowest MSE is going as low as 28.1 for lambda = 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f1406-02d0-4c84-a9cb-ae1c7f935df0",
   "metadata": {},
   "source": [
    "### 7 - Gradient descent, gradient of MSE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "904d6bb1-6adf-4bda-a617-0aae56e51dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.18537597878502\n"
     ]
    }
   ],
   "source": [
    "eta = 0.000001\n",
    "n_iterations = 100000\n",
    "\n",
    "theta7 = np.random.randn(X.shape[1])\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    y_pred7 = predict(X, theta7)\n",
    "    grad = gradient(X, y, y_pred7, theta7)\n",
    "    theta7 = theta7 - eta * grad\n",
    "\n",
    "scores7 = []\n",
    "for train, test in kf.split(kf3):\n",
    "    X_train = kf3[train, :-1]\n",
    "    y_train = kf3[train, -1]\n",
    "    X_test = kf3[test, :-1]\n",
    "    y_test = kf3[test, -1]\n",
    "    \n",
    "    theta_temp = np.random.randn(X.shape[1])\n",
    "    for iteration in range(n_iterations):\n",
    "        y_pred_temp = predict(X_train, theta_temp)\n",
    "        grad_temp = gradient(X_train, y_train, y_pred_temp, theta_temp)\n",
    "        theta_temp = theta_temp - eta * grad_temp\n",
    "\n",
    "    y_pred_temp = predict(X_test, theta_temp)\n",
    "    diff = y_pred_temp - y_test\n",
    "    MSE = sum(diff ** 2) / diff.size\n",
    "    scores7.append(MSE)\n",
    "\n",
    "print(np.average(scores7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d58ff-6095-4af3-9993-ac6570c9d131",
   "metadata": {},
   "source": [
    "After experimenting with the learning rate and n_iterations values, I settled on these values as they give the smallest error (relative to the linear regression closed-form solution). Still, this method was quite unstable, partially due to the random vector assumption but mostly due to the difficulty of picking a specific learning rate or maximum number of iterations with no mathematical equations helping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f463e485-9ffa-443c-82b2-7d78a5f38d93",
   "metadata": {},
   "source": [
    "### 8 - Gradient descent, lasso gradient (additional l1-penalty term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3190b429-82a8-4fa6-96c4-d3afc77fde72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.822257157647805\n"
     ]
    }
   ],
   "source": [
    "theta8 = np.random.randn(X.shape[1])\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    y_pred8 = predict(X, theta8)\n",
    "    grad = lasso_gradient(X, y, y_pred8, theta8)\n",
    "    theta8 = theta8 - eta * grad\n",
    "\n",
    "# print(theta3)\n",
    "# print(theta7 - theta3)\n",
    "\n",
    "scores8 = []\n",
    "for train, test in kf.split(kf3):\n",
    "    X_train = kf3[train, :-1]\n",
    "    y_train = kf3[train, -1]\n",
    "    X_test = kf3[test, :-1]\n",
    "    y_test = kf3[test, -1]\n",
    "    \n",
    "    theta_temp = np.random.randn(X.shape[1])\n",
    "    for iteration in range(n_iterations):\n",
    "        y_pred_temp = predict(X_train, theta_temp)\n",
    "        grad_temp = lasso_gradient(X_train, y_train, y_pred_temp, theta_temp)\n",
    "        theta_temp = theta_temp - eta * grad_temp\n",
    "\n",
    "    y_pred_temp = predict(X_test, theta_temp)\n",
    "    diff = y_pred_temp - y_test\n",
    "    MSE = sum(diff ** 2) / diff.size\n",
    "    scores8.append(MSE)\n",
    "\n",
    "print(np.average(scores8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd43ce-49a0-40f2-bac4-e2886866a8db",
   "metadata": {},
   "source": [
    "### 9 - Gradient descent, elastic gradient (additional l1-penalty and l2-penalty terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5fc1cf1e-d32f-4590-b569-05bd94584d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.2775649650509\n"
     ]
    }
   ],
   "source": [
    "theta9 = np.random.randn(X.shape[1])\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    y_pred9 = predict(X, theta8)\n",
    "    grad = elastic_gradient(X, y, y_pred9, theta9)\n",
    "    theta9 = theta9 - eta * grad\n",
    "\n",
    "scores9 = []\n",
    "for train, test in kf.split(kf3):\n",
    "    X_train = kf3[train, :-1]\n",
    "    y_train = kf3[train, -1]\n",
    "    X_test = kf3[test, :-1]\n",
    "    y_test = kf3[test, -1]\n",
    "    \n",
    "    theta_temp = np.random.randn(X.shape[1])\n",
    "    for iteration in range(n_iterations):\n",
    "        y_pred_temp = predict(X_train, theta_temp)\n",
    "        grad_temp = elastic_gradient(X_train, y_train, y_pred_temp, theta_temp)\n",
    "        theta_temp = theta_temp - eta * grad_temp\n",
    "\n",
    "    y_pred_temp = predict(X_test, theta_temp)\n",
    "    diff = y_pred_temp - y_test\n",
    "    MSE = sum(diff ** 2) / diff.size\n",
    "    scores9.append(MSE)\n",
    "\n",
    "print(np.average(scores9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f6c29-5c07-4b06-880e-068da062e61c",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748b328-30d9-48a0-9af5-9d3ecd9688bd",
   "metadata": {},
   "source": [
    "After comparing the resulting MSE from each model, I see that polynomial regression was the most successful comparatively. Other than avoiding randomness that does not feature much mathematical rigor (so far), it also provided the least MSE values (although scaling the data could have potentially produced different results). Also, when comparing ridge, lasso, and elastic gradient, they seemed to jump around a lot. I believe with better initial guesses, learning rate, maximum number of iterations, and coefficients for lasso and elastic regression, all gradient descent have better potential, but it just happened that my experiment was a bit better with quadratic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

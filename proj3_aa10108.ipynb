{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d6de47-a47e-4256-b5fc-d47dd0196d82",
   "metadata": {},
   "source": [
    "# Abdullah Alhussni - aa10108\n",
    "## Applied Machine Learning - ENGR-UH 3332 - Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcdc085-8442-4eeb-bd6e-393c8bc8261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "iris_df = pd.read_csv(\"iris.csv\", names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    iris_df = iris_df.replace({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}).infer_objects()\n",
    "\n",
    "spam_df = pd.read_csv(\"spambase.csv\")\n",
    "spam_df.columns = list(spam_df.columns[:-1]) + [\"class\"] # adding the \"class\" is necessary for our node definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50de3d8c-80fa-4eed-a5d3-f8c9fdf2893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    def __init__(self, data, n_min):\n",
    "        self.columns = data.columns\n",
    "        self.n_classes = data[\"class\"].nunique()\n",
    "        self.list_of_classes = data[\"class\"].unique()\n",
    "        data = data.to_numpy()\n",
    "        self.features = data[:, :-1]\n",
    "        self.target = data[:, -1]\n",
    "        self.n_instances = self.features.shape[0]\n",
    "        self.n_features = self.features.shape[1]\n",
    "        self.n_min = n_min\n",
    "        self.min_per_leaf = self.n_instances * n_min // 100\n",
    "        self.branchr = None\n",
    "        self.branchl = None\n",
    "        self.split_feature = None\n",
    "        self.split_condition = None\n",
    "        freq = np.zeros(self.n_classes)\n",
    "        for i in range(self.n_classes):\n",
    "            condition = self.target == self.list_of_classes[i]\n",
    "            freq[i] = len(self.target[condition])\n",
    "        self.label = self.list_of_classes[np.argmax(freq)]\n",
    "\n",
    "    def putback(self, features, target): # puts any split/branch into a dataframe again so it can be reconstructed into a node\n",
    "        data = pd.DataFrame(features, columns = self.columns[:-1])\n",
    "        data['class'] = target\n",
    "        n_min = self.n_min\n",
    "        a = node(data, n_min)\n",
    "        a.min_per_leaf = self.min_per_leaf # makes sure the min_per_leaf doesn't change when we split due to n_instances changing,\n",
    "        # then min_per_leaf is determined by the root training node\n",
    "        return a\n",
    "\n",
    "    def condition_split(self, split_feature, split_condition):\n",
    "        if split_feature != None or split_condition != None:\n",
    "            if self.features.dtype == 'bool':\n",
    "                condition = self.features == True\n",
    "            else:\n",
    "                condition = self.features[:, split_feature] >= split_condition\n",
    "\n",
    "            branch1features = self.features[np.where(condition)[0]]\n",
    "            branch2features = self.features[np.where(~condition)[0]]\n",
    "            branch1target = self.target[np.where(condition)[0]]\n",
    "            branch2target = self.target[np.where(~condition)[0]]\n",
    "\n",
    "            if len(branch1target) != 0 and len(branch2target) != 0:\n",
    "                branchr = self.putback(branch1features, branch1target)\n",
    "                branchl = self.putback(branch2features, branch2target)\n",
    "                self.branchr = branchr\n",
    "                self.branchl = branchl\n",
    "                self.branchr.min_per_leaf = self.min_per_leaf\n",
    "                self.branchl.min_per_leaf = self.min_per_leaf\n",
    "\n",
    "    def optimal_split(self):\n",
    "        IG_max = 0\n",
    "        best_feature = None\n",
    "        best_split_value = None\n",
    "        if self.n_classes > 1 and self.n_instances > self.min_per_leaf:\n",
    "            for i in range(self.n_features):\n",
    "                if self.features[:, i].dtype == 'bool':\n",
    "                    IG = self.IG(i, True)\n",
    "                    self.condition_split(i, True)\n",
    "                    if self.branchr == None or self.branchl == None:\n",
    "                        return None, None # the recursive_split function tests the output of optimal_split and if they're both\n",
    "                        # self, we understand that the optimal split is no split, so just return self in recursive_split\n",
    "                    if IG > IG_max:\n",
    "                        IG_max = IG\n",
    "                        best_feature = i\n",
    "                        best_split_value = True\n",
    "                else:\n",
    "                    a = np.unique(self.features[:, i])\n",
    "                    for j in range(len(a) - 1):\n",
    "                        b = (a[j] + a[j + 1]) / 2\n",
    "                        IG = self.IG(i, b)\n",
    "                        self.condition_split(i, b)\n",
    "                        if self.branchr == None or self.branchl == None:\n",
    "                            continue\n",
    "                        if IG > IG_max:\n",
    "                            IG_max = IG\n",
    "                            best_feature = i\n",
    "                            best_split_value = b\n",
    "            return best_feature, best_split_value\n",
    "        else:\n",
    "            return None, None # same as above\n",
    "\n",
    "    def recursive_split(self):\n",
    "        best_split_feature, best_split_condition = self.optimal_split()\n",
    "        self.split_feature = best_split_feature\n",
    "        self.split_condition = best_split_condition\n",
    "        self.condition_split(best_split_feature, best_split_condition)\n",
    "        if self.branchr != None and self.branchl != None:\n",
    "            self.branchl.recursive_split(), self.branchr.recursive_split()\n",
    "        elif self.branchr != None:\n",
    "            self.branchr.recursive_split()\n",
    "        elif self.branchl != None:\n",
    "            self.branchl.recursive_split()\n",
    "\n",
    "    def entropy(self):\n",
    "        probs = np.zeros(self.n_classes)\n",
    "        for i in range(self.n_classes):\n",
    "            condition = self.target == self.list_of_classes[i]\n",
    "            probs[i] = len(self.target[condition])\n",
    "        probs = probs / self.n_instances\n",
    "        probs = np.where(probs == 0, 1, probs) # replacing any probability 0 with 1 is to avoid log2(0)\n",
    "        # which returns the same term, 0\n",
    "        entropy = np.sum(-np.dot(probs, np.log2(probs)))\n",
    "        return entropy\n",
    "\n",
    "    def IG(self, split_feature, split_condition):\n",
    "        self.condition_split(split_feature, split_condition)\n",
    "        if self.branchr == None or self.branchl == None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.entropy() - (self.branchr.n_instances * self.branchr.entropy() + self.branchl.n_instances * self.branchl.entropy()) / (self.n_instances)\n",
    "\n",
    "    def create_decision_tree(self):\n",
    "        self.recursive_split()\n",
    "        print_binary_tree(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7f5c6c-b726-45a9-abd7-14b01a1163e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_binary_tree(root, indent = \"\", last = 'updown'):\n",
    "    if root is not None:\n",
    "        nb_spaces = \"   \"\n",
    "        if last == 'up':\n",
    "            if root.split_feature != None and root.split_condition != None:\n",
    "                print(f\"{indent}┌──Label: {root.label}, Feature_{root.split_feature} >= {root.split_condition}\")\n",
    "                indent += nb_spaces\n",
    "            else:\n",
    "                print(f\"{indent}┌──Label: {root.label}\")\n",
    "                indent += nb_spaces\n",
    "        elif last == 'down':\n",
    "            if root.split_feature != None and root.split_condition != None:\n",
    "                print(f\"{indent}└──Label: {root.label}, Feature_{root.split_feature} >= {root.split_condition}\")\n",
    "                indent += nb_spaces\n",
    "            else:\n",
    "                print(f\"{indent}└──Label: {root.label}\")\n",
    "                indent += nb_spaces\n",
    "        elif last == 'updown':\n",
    "            if root.split_feature != None and root.split_condition != None:\n",
    "                print(f\"{indent}Label: {root.label}, Feature_{root.split_feature} >= {root.split_condition}\")\n",
    "                indent += nb_spaces\n",
    "            else:\n",
    "                print(f\"{indent}Label: {root.label}\")\n",
    "                indent += nb_spaces\n",
    "\n",
    "        print_binary_tree(root.branchl, indent, 'up')\n",
    "        print_binary_tree(root.branchr, indent, 'down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06797655-e281-412c-992f-06389a7c8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, point): # the argument is node but technically it's the tree, but you should call recursive_split() first\n",
    "    position = node\n",
    "    while True:\n",
    "        if position.branchr == None or position.branchl == None:\n",
    "            return position.label\n",
    "        else:\n",
    "            i = position.split_feature\n",
    "            j = position.split_condition\n",
    "            if j == True:\n",
    "                position = position.branchr\n",
    "            elif j == False:\n",
    "                position = position.branchl\n",
    "            elif point[i] >= j:\n",
    "                position = position.branchr\n",
    "            else:\n",
    "                position = position.branchl\n",
    "\n",
    "def accuracy_test(node, test):\n",
    "    X_test = test.features\n",
    "    y_test = test.target\n",
    "    df = np.column_stack((X_test, y_test))\n",
    "    num = test.n_instances\n",
    "    y_pred = []\n",
    "    for i in range(num):\n",
    "        y_pred.append(predict(node, df[i, :]))\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df88350a-82ec-41cc-8a93-c5e364226ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Decision Tree\n",
      "Label: 0, Feature_2 >= 2.45\n",
      "   ┌──Label: 0.0\n",
      "   └──Label: 1.0, Feature_3 >= 1.75\n",
      "      ┌──Label: 1.0, Feature_2 >= 4.95\n",
      "         ┌──Label: 1.0, Feature_3 >= 1.65\n",
      "            ┌──Label: 1.0\n",
      "            └──Label: 2.0\n",
      "         └──Label: 2.0\n",
      "      └──Label: 2.0, Feature_2 >= 4.85\n",
      "         ┌──Label: 2.0\n",
      "         └──Label: 2.0\n",
      "Spam Dataset Decision Tree\n",
      "Label: 0, Feature_52 >= 0.0555\n",
      "   ┌──Label: 0.0, Feature_6 >= 0.055\n",
      "      ┌──Label: 0.0, Feature_51 >= 0.191\n",
      "         ┌──Label: 0.0, Feature_24 >= 0.025\n",
      "            ┌──Label: 0.0, Feature_55 >= 10.5\n",
      "               ┌──Label: 0.0, Feature_15 >= 0.845\n",
      "                  ┌──Label: 0.0, Feature_7 >= 0.035\n",
      "                     ┌──Label: 0.0, Feature_26 >= 0.125\n",
      "                        ┌──Label: 0.0, Feature_18 >= 0.935\n",
      "                           ┌──Label: 0.0\n",
      "                           └──Label: 0.0\n",
      "                        └──Label: 0.0\n",
      "                     └──Label: 0.0\n",
      "                  └──Label: 0.0\n",
      "               └──Label: 0.0, Feature_4 >= 0.74\n",
      "                  ┌──Label: 0.0, Feature_44 >= 0.355\n",
      "                     ┌──Label: 0.0\n",
      "                     └──Label: 0.0\n",
      "                  └──Label: 1.0\n",
      "            └──Label: 0.0, Feature_10 >= 1.115\n",
      "               ┌──Label: 0.0, Feature_15 >= 2.295\n",
      "                  ┌──Label: 0.0, Feature_22 >= 0.015\n",
      "                     ┌──Label: 0.0, Feature_25 >= 0.395\n",
      "                        ┌──Label: 0.0\n",
      "                        └──Label: 0.0\n",
      "                     └──Label: 0.0\n",
      "                  └──Label: 1.0\n",
      "               └──Label: 1.0\n",
      "         └──Label: 0.0, Feature_54 >= 2.7655000000000003\n",
      "            ┌──Label: 0.0\n",
      "            └──Label: 1.0\n",
      "      └──Label: 1.0\n",
      "   └──Label: 1.0, Feature_24 >= 0.4\n",
      "      ┌──Label: 1.0, Feature_45 >= 0.49\n",
      "         ┌──Label: 1.0, Feature_55 >= 9.5\n",
      "            ┌──Label: 1.0\n",
      "            └──Label: 1.0, Feature_51 >= 0.143\n",
      "               ┌──Label: 1.0\n",
      "               └──Label: 1.0, Feature_45 >= 0.275\n",
      "                  ┌──Label: 1.0, Feature_49 >= 0.0695\n",
      "                     ┌──Label: 1.0\n",
      "                     └──Label: 1.0\n",
      "                  └──Label: 1.0\n",
      "         └──Label: 0.0\n",
      "      └──Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_min_iris = [5, 10, 15, 20]\n",
    "n_min_spam = [5, 10, 15, 20, 25]\n",
    "\n",
    "iris = node(iris_df, n_min_iris[1])\n",
    "\n",
    "print(\"Iris Dataset Decision Tree\")\n",
    "iris.create_decision_tree()\n",
    "\n",
    "spam = node(spam_df, n_min_spam[1])\n",
    "\n",
    "print(\"Spam Dataset Decision Tree\")\n",
    "spam.create_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d498e-3303-409e-a0dc-0372c92d2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Decision Tree Accuracy for Different Values of n_min\n",
      "   n_min Mean Accuracy Standard Deviation\n",
      "0      5        94.67%              5.27%\n",
      "1     10        94.67%              5.27%\n",
      "2     15        94.67%              5.27%\n",
      "3     20        94.67%              5.27%\n",
      "Spam Dataset Decision Tree Accuracy for Different Values of n_min\n"
     ]
    }
   ],
   "source": [
    "k = 10    \n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = seed)\n",
    "\n",
    "def decision_tree_accuracy(df, n_min):\n",
    "    data = node(df, n_min)\n",
    "    df = np.column_stack((data.features, data.target))\n",
    "    accuracies = []\n",
    "    for train, test in kf.split(df):\n",
    "        X_train = df[train, :-1]\n",
    "        y_train = df[train, -1]\n",
    "        X_test = df[test, :-1]\n",
    "        y_test = df[test, -1]\n",
    "\n",
    "        data_train = data.putback(X_train, y_train)\n",
    "        data_test = data.putback(X_test, y_test)\n",
    "\n",
    "        data_train.recursive_split()\n",
    "        accuracy = accuracy_test(data_train, data_test)\n",
    "        accuracies.append(accuracy)\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        stdev = np.std(accuracies)\n",
    "    return mean_accuracy, stdev\n",
    "\n",
    "def decision_tree_table(df, n_min):\n",
    "    accuracies = []\n",
    "    stdevs = []\n",
    "    \n",
    "    for n in n_min:\n",
    "        x, y = decision_tree_accuracy(df, n)\n",
    "        accuracies.append(x)\n",
    "        stdevs.append(y)\n",
    "\n",
    "    # Convert mean accuracies to percentages\n",
    "    accuracy_percent = [f\"{accuracy * 100:.2f}%\" for accuracy in accuracies]\n",
    "    \n",
    "    # Convert stdevs to percentage of the mean accuracy\n",
    "    stdev_percent = [f\"{(stdev / accuracies[i]) * 100:.2f}%\" for i, stdev in enumerate(stdevs)]\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'n_min': n_min,\n",
    "        'Mean Accuracy': accuracy_percent,\n",
    "        'Standard Deviation': stdev_percent\n",
    "    })\n",
    "    \n",
    "    print(results_df)\n",
    "\n",
    "print(\"Iris Dataset Decision Tree Accuracy for Different Values of n_min\")\n",
    "decision_tree_table(iris_df, n_min_iris)\n",
    "print(\"Spam Dataset Decision Tree Accuracy for Different Values of n_min\")\n",
    "decision_tree_table(spam_df, n_min_spam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
